{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "1 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "2 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "3 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "4 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "5 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "6 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "7 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "8 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "9 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "10 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "11 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "12 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "13 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "14 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "15 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "16 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "17 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "18 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "19 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "20 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "21 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "22 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "23 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "24 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "25 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "26 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "27 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "28 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "29 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "30 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "31 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "32 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "33 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "34 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "35 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "36 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "37 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "38 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "39 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "40 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "41 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "42 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "43 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "44 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "45 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "46 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "47 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "48 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "49 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "50 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "51 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "52 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "53 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "54 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "55 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "56 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "57 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "58 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "59 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "60 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "61 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "62 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "63 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "64 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "65 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "66 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "67 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "68 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "69 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "70 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "71 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "72 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "73 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "74 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "75 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "76 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "77 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "78 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "79 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "80 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "81 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "82 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "83 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "84 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "85 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "86 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "87 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "88 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "89 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "90 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "91 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "92 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "93 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "94 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "95 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "96 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "97 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "98 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "99 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "100 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "101 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "102 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "103 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "104 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "105 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "106 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "107 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "108 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "109 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "110 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "111 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "112 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "113 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "114 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "115 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "116 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "117 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "118 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "119 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "120 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "121 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "122 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "123 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "124 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "125 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "126 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "127 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "128 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "129 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "130 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "131 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "132 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "133 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "134 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "135 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "136 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "137 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "138 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "139 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "140 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "141 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "142 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "143 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "144 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "145 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "146 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "147 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "148 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "149 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "150 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "151 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "152 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "153 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "154 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "155 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "156 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "157 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "158 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "159 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "160 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "161 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "162 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "163 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "164 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "165 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "166 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "167 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "168 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "169 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "170 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "171 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "173 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "174 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "175 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "176 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "177 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "178 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "179 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "180 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "181 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "182 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "183 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "184 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "185 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "186 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "187 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "188 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "189 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "190 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "191 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "192 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "193 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "194 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "195 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "196 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "197 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "198 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "199 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "200 4.6166506 [[ 0.9617566   1.1860163  -0.5166012 ]\n",
      " [-1.4726235   0.07167324 -0.6077964 ]\n",
      " [-0.4629606  -0.02906075 -0.25153285]]\n",
      "Prediction :  [1 1 1]\n",
      "Accuracy :  0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2,1],[1,3,2],[1,3,4],[1,5,5],[1,7,5],[1,2,5],[1,6,6],[1,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "#test data를 꼭 따로 둬야 함\n",
    "x_test = [[2,1,1],[3,1,2],[3,3,4]]\n",
    "y_test = [[0,0,1],[0,0,1],[0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# 테스트 모델의 정확성 측정\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "is_correct = tf.equal(prediction,tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess :\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost,W,optimizer],\n",
    "                                     feed_dict={ X: x_data,Y:y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    print(\"Prediction : \",sess.run(prediction,feed_dict={X:x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy : \",sess.run(accuracy,feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost:  122291360000.0 \n",
      "Prediction:\n",
      " [[246595.17]\n",
      " [498488.28]\n",
      " [391721.28]\n",
      " [274004.22]\n",
      " [323278.  ]\n",
      " [326018.34]\n",
      " [298657.4 ]\n",
      " [380797.1 ]]\n",
      "500 cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1000 cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "1500 cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "2000 cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "W = tf.Variable(tf.random_normal([4,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X,W)+b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost,hypothesis,train],feed_dict={X:x_data,Y:y_data})\n",
    "    if step % 500 == 0 : \n",
    "        print(step,\"cost: \",cost_val,\"\\nPrediction:\\n\",hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 cost:  0.21123427 \n",
      "Prediction:\n",
      " [[ 0.69433653]\n",
      " [-0.10153033]\n",
      " [ 0.104059  ]\n",
      " [ 0.23923938]\n",
      " [ 0.3009987 ]\n",
      " [ 0.1864001 ]\n",
      " [ 0.10081448]\n",
      " [-0.49847525]]\n",
      "500 cost:  0.20525241 \n",
      "Prediction:\n",
      " [[ 0.7039525 ]\n",
      " [-0.09151272]\n",
      " [ 0.11225639]\n",
      " [ 0.245394  ]\n",
      " [ 0.30837482]\n",
      " [ 0.19355105]\n",
      " [ 0.10565315]\n",
      " [-0.49347144]]\n",
      "1000 cost:  0.19949603 \n",
      "Prediction:\n",
      " [[ 0.7133691 ]\n",
      " [-0.08168662]\n",
      " [ 0.12029546]\n",
      " [ 0.25142664]\n",
      " [ 0.31560677]\n",
      " [ 0.20056227]\n",
      " [ 0.11040151]\n",
      " [-0.4885599 ]]\n",
      "1500 cost:  0.1939554 \n",
      "Prediction:\n",
      " [[ 0.7225937 ]\n",
      " [-0.07204565]\n",
      " [ 0.12817958]\n",
      " [ 0.25733963]\n",
      " [ 0.32269567]\n",
      " [ 0.20743576]\n",
      " [ 0.115057  ]\n",
      " [-0.48373577]]\n",
      "2000 cost:  0.18861601 \n",
      "Prediction:\n",
      " [[ 0.7316471 ]\n",
      " [-0.0625742 ]\n",
      " [ 0.13592213]\n",
      " [ 0.26314214]\n",
      " [ 0.3296553 ]\n",
      " [ 0.21418393]\n",
      " [ 0.11962632]\n",
      " [-0.4789991 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "xy = min_max_scaler(xy) \n",
    "#가장 큰 값을 1, 가장 작은 값을 0으로 잡은 다음 그 사이 값을 0~1 사이의 범위의 숫자로\n",
    "#데이터를 normalized 해 줌 -> min_max_scaler\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None,4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "W = tf.Variable(tf.random_normal([4,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X,W)+b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train=optimizer.minimize(cost)\n",
    "\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost,hypothesis,train],feed_dict={X:x_data,Y:y_data})\n",
    "    if step % 500 == 0 : \n",
    "        print(step,\"cost: \",cost_val,\"\\nPrediction:\\n\",hy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.787137681\n",
      "Epoch: 0002 cost = 1.083853499\n",
      "Epoch: 0003 cost = 0.866050942\n",
      "Epoch: 0004 cost = 0.759799681\n",
      "Epoch: 0005 cost = 0.693108506\n",
      "Epoch: 0006 cost = 0.645002258\n",
      "Epoch: 0007 cost = 0.608306037\n",
      "Epoch: 0008 cost = 0.579890587\n",
      "Epoch: 0009 cost = 0.555573721\n",
      "Epoch: 0010 cost = 0.535227553\n",
      "Epoch: 0011 cost = 0.517350626\n",
      "Epoch: 0012 cost = 0.501956862\n",
      "Epoch: 0013 cost = 0.488419924\n",
      "Epoch: 0014 cost = 0.476560553\n",
      "Epoch: 0015 cost = 0.465506965\n",
      "Accuracy :  0.8906\n",
      "Label :  [2]\n",
      "Prediction :  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#MNIST 손글씨 데이터 로드, 읽었을 때 바로 one hot으로 처리 가능\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "\n",
    "nb_classes =10 #클래스 10개 의미\n",
    "# MNIST data image of shape 28*28 = 784 (이미지 데이터가 28*28 픽셀 안에 있음)\n",
    "X = tf.placeholder(tf.float32,[None, 784])\n",
    "# 0 - 9 수. 10개의 출력이 나타나요\n",
    "Y = tf.placeholder(tf.float32,[None,nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "#softmax 사용\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "#합을 구해서 평균으로 내기\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1)) \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis,1),tf.arg_max(Y,1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "# parameters\n",
    "#데이터의 양이 너무 많아서 한 번에 다 읽을 수 없기 때문에 batch를 이용해 조금씩 잘라옴\n",
    "training_epochs = 15 # 1 epoch = 전체 데이터를 한 번 다 학습시킨 것\n",
    "batch_size = 100 #한 번에 학습할 양. 100개의 training data 읽어오기\n",
    "with tf.Session() as sess :\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        #전체 데이터 개수를 batch 사이즈로 나눔.\n",
    "        #(전체 데이터가 10000개고, batch 사이즈가 100일 때 100번 돌면 1epoch을 돌게 됨)\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        #이 루프를 한 번 돌 때마다 1 epoch가 끝나는 것을 의미함\n",
    "        for i in range(total_batch): # iteration\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        print('Epoch:','%04d'%(epoch+1),'cost =','{:.9f}'.format(avg_cost))\n",
    "    \n",
    "    # Training data가 아닌 test data를 이용하여 정확도 측정\n",
    "    # sess.run() 으로 돌릴 수도 있고, accuracy.eval() <-으로 할 수도 있음\n",
    "    print(\"Accuracy : \",accuracy.eval(session=sess,\n",
    "                                      feed_dict={X:mnist.test.images,Y:mnist.test.labels}))\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0,mnist.test.num_examples -1)\n",
    "    print(\"Label : \",sess.run(tf.argmax(mnist.test.labels[r:r+1],1)))\n",
    "    print(\"Prediction : \",sess.run(tf.argmax(hypothesis,1),feed_dict={X:mnist.test.images[r:r+1]}))\n",
    "    plt.imshow(mnist.test.images[r:r+1].\n",
    "              reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
